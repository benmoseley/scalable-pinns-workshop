{"cells":[{"cell_type":"markdown","metadata":{"id":"F9PiNvd5Up0A"},"source":["# Session 2 - Practical: Introduction to JAX and coding a PINN from scratch in JAX\n","\n","## Goal\n","\n","The goal of this practical is to:\n","- Learn the basics of JAX, including its core concept of **function transformation**\n","- Use these basic ideas to train a **physics-informed neural network** (PINN) **from scratch** in JAX, and **compare its performance** to the PyTorch version trained in Session 1.\n","\n","## Contents\n","\n","This practical has two parts:\n","- Part 1: Introduction to **JAX**\n","- Part 2: Training a **PINN** to solve the 1D harmonic oscillator equation.\n","\n","Please see the lecture slides from Session 1 and 2 for more explanation on these topics."]},{"cell_type":"markdown","metadata":{"id":"qsfTGl9cdm-H"},"source":["## Part 1: Introduction to **JAX**"]},{"cell_type":"markdown","metadata":{"id":"18eXb0AWdtt0"},"source":["### Step 1: Code-along with Lecture\n","\n","This step is a code-along; please follow along with the lecture."]},{"cell_type":"markdown","metadata":{"id":"5d15ed39"},"source":["#### What is JAX?\n","\n","<img src=\"https://raw.githubusercontent.com/benmoseley/intro-to-jax-workshop/main/what-is-jax.png\" width=80%>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5560b2ba"},"outputs":[],"source":["import os\n","os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=2'\n","import jax\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"c8302346"},"source":["#### Arrays with JAX NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4ad4857"},"outputs":[],"source":["import jax.numpy as jnp\n","\n","x = jnp.array([[0., 2., 4.]])\n","print(x, x.shape)\n","print(x @ x.T)\n","print(x * x.T)"]},{"cell_type":"markdown","metadata":{"id":"3a148018"},"source":["#### Autodifferentiation with JAX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91bf0780"},"outputs":[],"source":["def fn(x):\n","    return jnp.tanh(x)\n","\n","x = jnp.linspace(-5,5,500)\n","\n","plt.plot(x, fn(x), label=\"f(x)\")\n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e0c8271"},"outputs":[],"source":["# gradient\n","dfdx_fn =\n","d2fdx2_fn =\n","\n","plt.plot(x, fn(x), label=\"f(x)\")\n","plt.plot(x, [dfdx_fn(x_) for x_ in x], label=\"df/dx\")\n","plt.plot(x, [d2fdx2_fn(x_) for x_ in x], label=\"d$^2$f/dx$^2$\")\n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7d43fe8"},"outputs":[],"source":["print(jax.make_jaxpr(fn)(x))# JAX transforms programs using a simple intermediate language call jaxpr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"df6a6ae1"},"outputs":[],"source":["# Jacobian\n","jacobian_fn = jax.jacfwd(fn)\n","j = jacobian_fn(x)\n","print(j)\n","print(j.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7506a9d4"},"outputs":[],"source":["# vector-Jacobian product\n","f, vjp_fn = jax.vjp(fn, x)\n","dfdx, = vjp_fn(jnp.ones_like(x))\n","\n","plt.plot(x, f, label=\"f(x)\")\n","plt.plot(x, dfdx, label=\"df/dx\")\n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.show()\n","\n","# Jacobian-vector product\n","f, dfdx = jax.jvp(fn, (x,), (jnp.ones_like(x),))\n","\n","plt.plot(x, f, label=\"f(x)\")\n","plt.plot(x, dfdx, label=\"df/dx\")\n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"34db3256"},"source":["#### Vectorisation with JAX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e18a0da9"},"outputs":[],"source":["def forward_fn(w, b, x):\n","    x = w @ x + b\n","    x = jnp.tanh(x)\n","    return x\n","\n","key = jax.random.key(seed=0)\n","key1, key2, key3 = jax.random.split(key, 3)\n","x = jax.random.normal(key1, (3,))\n","w = jax.random.normal(key2, (10,3))\n","b = jax.random.normal(key3, (10,))\n","y = forward_fn(w, b, x)\n","print(x.shape)\n","print(y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d080ab23"},"outputs":[],"source":["forward_batch_fn =\n","\n","x_batch = jax.random.normal(key, (1000,3))\n","y_batch = forward_batch_fn(w, b, x_batch)\n","print(x_batch.shape)\n","print(y_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"6c05563f"},"source":["#### Just-in-time compilation with JAX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cba98473"},"outputs":[],"source":["def fn(x):\n","    return x + x*x + x*x*x\n","\n","jit_fn =\n","\n","x = jax.random.normal(key, (1000,1000))\n","%timeit fn(x).block_until_ready()\n","%timeit jit_fn(x).block_until_ready()"]},{"cell_type":"markdown","metadata":{"id":"b8f3e59e"},"source":["### Step 2: Your first JAX code: carrying out polynominal regression\n","\n","The goal of this task is to gain a basic familiarity with JAX's function transforms (`vmap`, `jit`, `grad`).\n","\n","We will carry out polynominal regression, using the following data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d79f9bdc"},"outputs":[],"source":["key = jax.random.key(0)\n","x_batch = jnp.linspace(-1, 1, 100).reshape((100,1))\n","y_label_batch = 3*x_batch**3 - x_batch**2 - 3*x_batch + 2 + 0.2*jax.random.normal(key, (100,1))\n","\n","plt.scatter(x_batch, y_label_batch, label=\"training data\")\n","plt.legend()\n","plt.xlabel(\"x\"); plt.ylabel(\"y\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"d0fd01d4"},"source":["> Task 1: define a `forward(theta, x)` function, which predicts `y` given a single `x` value and some learnable parameters, `theta`.\n","\n","> Task 2: use `jax.vmap` to transform `forward` to a function which predicts a batch of `y` values given a batch of `x` values. Test that it works using `x_batch` above.\n","\n","Hint: you can assume that the function is a third order polynominal, and learn its coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3674a5f8"},"outputs":[],"source":["def forward(theta, x):\n","    \"Returns model prediction, for a single example input\"\n","    y = # TODO\n","    return y\n","\n","forward_batch = # TODO: batched version of forward\n","\n","theta = jnp.zeros(4, dtype=float)\n","print(forward_batch(theta, x_batch).shape)"]},{"cell_type":"markdown","metadata":{"id":"a4be3d34"},"source":["> Task 3: next, define a `loss(theta, x_batch, y_label_batch)` function which computes the mean-squared error of the model.\n","\n","> Task 4: use `jax.grad` to transform `loss` into a function which computes the gradient of `loss` with respect to the model parameters, `theta`. Test that it outputs an array the same shape as `theta`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ddd0f69"},"outputs":[],"source":["def loss(theta, x_batch, y_label_batch):\n","    \"Computes mean squared error between model prediction and training data\"\n","    loss = # TODO\n","    return loss\n","\n","grad = # TODO: gradient of loss wrt model parameters\n","\n","print(grad(theta, x_batch, y_label_batch)[1])"]},{"cell_type":"markdown","metadata":{"id":"7ca0f14c"},"source":["> Task 5: define an `step(lrate, theta, x_batch, y_label_batch)` function which carries out a single gradient descent step on `theta`, using the `grad` function you created above. It should return the updated `theta`.\n","\n","> Task 6: use `jax.jit` to compile `step`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80e66b9c"},"outputs":[],"source":["def step(lrate, theta, x_batch, y_label_batch):\n","    \"Performs one gradient descent step on model parameters, given training data\"\n","    # TODO\n","    return theta, lossval\n","\n","jit_step = # TODO"]},{"cell_type":"markdown","metadata":{"id":"9dfef057"},"source":["> Task 7: finally, write a python `for` loop to train the model over 1000 gradient descent steps, with a learning rate of 0.1. You are free to choose how to initialise `theta`. Plot the results of the model.\n","\n","> Task 8: compare the speed of training with/without `jit` compilation. How much faster is the compiled code?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfa411a4"},"outputs":[],"source":["# initialise theta\n","theta = # TODO\n","\n","# run gradient descent\n","for i in range(1000):\n","    # TODO\n","\n","# TODO: plot the results of the model, and compare training speed with/without `jit`"]},{"cell_type":"markdown","metadata":{"id":"f1e53e15"},"source":["### Extra note: ðŸ”ª JAX: the sharp bits\n","\n","JAX introduces a number of **restrictions** which are necessary for JAX transformations to work (`vmap`, `grad`, `jit`, etc).\n","\n","Below are most of the major ones. See [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for a full list."]},{"cell_type":"markdown","metadata":{"id":"FefSESYMd9CU"},"source":["#### ðŸ”ª Pure functions\n","\n","JAX transforms are designed to work on pure functions.\n","\n","That is, all the input data is passed through the function parameters, and all the results are output through the function results.\n","\n","Otherwise unexpect results might occur! Note: this makes transforming Python classes tricky (but not [impossible](https://jax.readthedocs.io/en/latest/stateful-computations.html))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"149e1919"},"outputs":[],"source":["x = 10\n","def f(a):\n","    return x+a\n","\n","print(jax.make_jaxpr(f)(1))# when we jit this function, the current value of x is hard-coded into the compiled code!\n","jit_f = jax.jit(f)\n","print(jit_f(1))\n","x = 20\n","print(jit_f(1))# (!) not what you expect (!)"]},{"cell_type":"markdown","metadata":{"id":"b0e2fa79"},"source":["#### ðŸ”ªðŸ”ª Static shapes\n","JAX transforms require all output shapes to be known in **advance**, given the input shapes.\n","\n","This is probably the biggest restriction. We can't transform functions which output dynamic shapes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c867819c"},"outputs":[],"source":["def f(i):\n","    return jnp.ones(shape=(i,))\n","\n","print(f(jnp.array(10)))\n","#print(jax.jit(f)(jnp.array(10)))# raises a TypeError"]},{"cell_type":"markdown","metadata":{"id":"1f318311"},"source":["ðŸ”ª This means **dynamic programming** and **sparse (matrix) computation** can often be challenging in JAX (although tricks like masking and compressed representations can be used)."]},{"cell_type":"markdown","metadata":{"id":"12f8cc10"},"source":["#### ðŸ”ª Out-of-place array updates\n","\n","JAX only allows **out-of-place** array updates.\n","\n","This is because allowing mutation of variables in-place makes program analysis and transformation difficult."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4e7b287"},"outputs":[],"source":["a = np.array([1,2,3])\n","a[0] = -1# you are used to doing this\n","\n","a = jnp.array([1,2,3])\n","#a[0] = 1# will raise a TypeError\n","b = a.at[0].set(-1)# this is the JAX equivalent\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"107352f3"},"source":["#### ðŸ”ª Random numbers\n","\n","JAX handles random number generation explicitly ([for good reason](https://jax.readthedocs.io/en/latest/random-numbers.html)).\n","\n","This means the user must explicitly carry around the state of the random number generator:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7efab6cf"},"outputs":[],"source":["key = jax.random.key(0)\n","print(jax.random.normal(key, (1,)))\n","print(jax.random.normal(key, (1,)))# generates same number\n","key, subkey = jax.random.split(key, num=2)# we need to explicitly split the RNG to generate new numbers\n","print(jax.random.normal(subkey, (1,)))"]},{"cell_type":"markdown","metadata":{"id":"GHVNPkY8U7GM"},"source":["## Part 2: Training a **PINN** to solve the 1D harmonic oscillator equation\n","\n","<div style=\"text-align: center; margin-top: 20px\">\n","    <img src=\"images/example_solution.png\" width=500px>\n","</div>\n","<div style=\"text-align: center;\">\n","    <img src=\"https://raw.githubusercontent.com/benmoseley/AISE-2024/main/5_introduction_to_PINNs/oscillator.gif\" width=500px>\n","</div>\n","<div style=\"text-align: center; font-weight: bold; margin-top: 5px; margin-bottom: 20px\">\n","    Fig 1: Example PINN solution (top) for the 1D harmonic oscillator problem (bottom)\n","</div>\n","\n","### Goal\n","\n","In this part, the goal is to **rewrite the PyTorch PINN code from Session 1 (training a PINN to solve the harmonic oscillator problem) into JAX**.\n","\n","We want to replicate the exact same hyperparmeters and setup, and test how JAX compares to PyTorch in terms of speed.\n","\n","We will be using all of the JAX functionality you learnt above; `jax.numpy`, `vmap`, `grad`, and `jit`, to write a performant PINN implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roCR83SgZAn_"},"outputs":[],"source":["import time\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","\n","import jax\n","import jax.numpy as jnp\n","import optax\n","\n","def exact_solution(d, w0, t):\n","    \"Computes the analytical solution to the under-damped harmonic oscillator.\"\n","    assert d < w0\n","    w = jnp.sqrt(w0**2-d**2)\n","    phi = jnp.arctan(-d/w)\n","    A = 1/(2*jnp.cos(phi))\n","    cos = jnp.cos(phi+w*t)\n","    exp = jnp.exp(-d*t)\n","    u = exp*2*A*cos\n","    return u\n","\n","def plot_result(i, t_physics_batch, t_boundary, t_test_batch, u_exact_batch, u_test_batch):\n","    \"Plots the PINN's prediction\"\n","    test_error = jnp.mean(jnp.abs(u_test_batch-u_exact_batch)/u_exact_batch.std())\n","    plt.figure(figsize=(8,3))\n","    plt.scatter(t_physics_batch[:,0], jnp.zeros_like(t_physics_batch)[:,0], s=20, lw=0, color=\"tab:blue\", alpha=0.6, label=\"Collocation points\")\n","    plt.scatter(t_boundary, 0, s=20, lw=0, color=\"tab:red\", alpha=0.6, label=\"Boundary point\")\n","    plt.plot(t_test_batch[:,0], u_exact_batch[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n","    plt.plot(t_test_batch[:,0], u_test_batch[:,0], label=\"PINN solution\", color=\"tab:green\")\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.title(f\"Training step {i+1}    Relative L1 test error: {test_error:.2%}\")\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PGl5knxuZvRL"},"source":["### Step 1: Defining a fully connected neural network in JAX\n","\n","The first step is to define a fully connected network (FCN) in JAX.\n","\n","> Task 1: complete the `FCN.forward` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mv_J_3gTYfN0"},"outputs":[],"source":["class FCN:\n","    \"\"\"Fully connected network in JAX, using only jax.numpy\n","\n","    Note for the curious reader: any functions which are designed for JAX transformations (vmap, grad, jit, etc)\n","    should not include side-effects (a side-effect is any effect of a function that doesnâ€™t appear in its output).\n","\n","    But standard python class methods (e.g. def __init__(self, ..)) often adjust `self` outside of the method.\n","    This is risky, unless you know exactly what you are doing; and is why we only define static methods here.\n","\n","    This means the FCN class is really just a collection of functions, i.e. a convienent namespace,\n","    and we carry around the state of the network (i.e. its `parameters`, normally contained in `self` in PyTorch)\n","    explicitly in our JAX code, passing it explicitly to each method. See here for more discussion:\n","    https://docs.jax.dev/en/latest/stateful-computations.html\n","    \"\"\"\n","\n","    @staticmethod\n","    def init_parameters(key, layer_sizes):\n","        \"\"\"Initialise the parameters of the network.\n","        Parameters:\n","            key: current JAX RNG state\n","            layer_sizes: list defining the number of layers and the number of channels per layer, including\n","            input/output layers, e.g. [1,16,16,1]\n","        Returns:\n","            parameters: list of randomly initialised weights and biases [(W0, b0), ...]\n","\n","        Note: JAX uses explicit random seed management; so we need to pass the current RNG state (`key`) explicitly\n","        to any `jax.random` calls, and split the RNG state by hand.\n","        \"\"\"\n","\n","        keys = jax.random.split(key, len(layer_sizes)-1)# split the key\n","        parameters = [FCN._random_layer_parameters(k, m, n)\n","                      for k, m, n in zip(keys, layer_sizes[:-1], layer_sizes[1:])]# initialise all layers\n","\n","        return parameters\n","\n","    @staticmethod\n","    def _random_layer_parameters(key, m, n):\n","        \"Randomly intialise the parameters of a single layer\"\n","\n","        W_key, b_key = jax.random.split(key)\n","        v = jnp.sqrt(1/m)# LeCun initialisation\n","        W = jax.random.uniform(W_key, (n, m), minval=-v, maxval=v)\n","        b = jax.random.uniform(b_key, (n,), minval=-v, maxval=v)\n","\n","        return W,b\n","\n","    @staticmethod\n","    def forward(parameters, x):\n","        \"\"\"Forward pass of the network.\n","        Parameters:\n","            parameters: list of weights and biases [(W0,b0), ...]\n","            x: SINGLE input point of shape (xdim=layer_sizes[0],)\n","        Returns:\n","            u: SINGLE output point of shape (udim=layer_sizes[-1],)\n","\n","        Note: a key philosophical difference between JAX and PyTorch is that in JAX we define non-batched\n","        operations (like the forward pass of the network with a SINGLE input point), and then use `vmap`\n","        afterwards to define the batched version (e.g. for evaluating the network over many points).\n","        With experience, this is a powerful way to build sophisticated, peformant, flexible code which remains\n","        readable and is less bug-prone.\n","        \"\"\"\n","\n","        assert x.ndim == 1\n","        activation_fn = jnp.tanh\n","\n","        ## Task: finish defining the forward function.\n","        ## Use tanh for hidden activations, and no activation for the last layer.\n","\n","\n","\n","        ##\n","\n","        assert u.ndim == 1\n","        return u"]},{"cell_type":"markdown","metadata":{"id":"DucDcZQCYgDW"},"source":["> Task 2: plot the output of the untrained network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu02PVNnWIck"},"outputs":[],"source":["# set random seed\n","key = jax.random.PRNGKey(0)\n","\n","# define the neural network architecture and initialise its parameters\n","network = FCN\n","parameters = network.init_parameters(key, [1,32,32,1])\n","\n","\n","## Task: plot the output of the untrained network\n","## hint: you can `vmap` network.forward to compute its output over a batch of input points\n","\n","\n","\n","##"]},{"cell_type":"markdown","metadata":{"id":"Wkw48UHkVfOV"},"source":["### Step 2: Computing the PINN loss function\n","\n","Next, we will use `jax.grad` to compute first and second order gradients of the FCN's output with respect to its input, and use these gradients to evaluate the PINN's loss function.\n","\n","Recall that the PINN loss function for the 1D harmonic oscillator problem is:\n","\n","$$\n","L(\\theta)= (\\hat{u}(0,\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d \\hat{u}}{dt}(0,\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N_I} \\sum^{N_I}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] \\hat{u}(t_{i},\\theta) Â \\right)^2~.\n","$$\n","\n","> Task 3: complete the `PINN_physics_loss`, `PINN_boundary_losses`, and `PINN_loss_batch` functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZcUL5wfWNpx"},"outputs":[],"source":["def PINN_physics_loss(parameters, t, network, mu, k):\n","    \"\"\"Compute the physics loss for the 1D harmonic oscillator problem.\n","    Parameters:\n","        parameters: network parameters\n","        t: SINGLE input point of shape (1,)\n","        network: network class\n","        mu: coefficient of friction\n","        k: spring constant\n","    Returns:\n","        loss_physics: SINGLE SCALAR loss value of shape ()\n","\n","    ODE:\n","    d^2 u      du\n","    ----- + mu -- + ku = 0\n","    dt^2       dt\n","\n","    Boundary conditions:\n","    u (0) = 1\n","    u'(0) = 0\n","    \"\"\"\n","\n","    assert t.shape == (1,)\n","\n","    def u_fn(t):\n","        \"\"\"Calls network.forward, isolating u as a scalar function of a scalar input\n","        suitable for jax.grad\"\"\"\n","        return network.forward(parameters, t.reshape(1,)).squeeze()# has shape ()\n","    t = t.squeeze()# has shape ()\n","\n","    u = u_fn(t)# has shape ()\n","\n","    ## Task: use `jax.grad` to compute the gradients of u_fn at t, dudt and d2udt2, and then use\n","    ## these to compute the physics loss\n","\n","\n","\n","    ##\n","\n","    assert loss_physics.shape == ()\n","    return loss_physics\n","\n","\n","def PINN_boundary_losses(parameters, t, network):\n","    \"\"\"Compute the two boundary losses for the 1D harmonic oscillator problem.\n","    Parameters:\n","        parameters: network parameters\n","        t: SINGLE input point of shape (1,)\n","        network: network class\n","    Returns:\n","        loss_displacement: SINGLE SCALAR loss value of shape ()\n","        loss_velocity: SINGLE SCALAR loss value of shape ()\n","\n","    ODE:\n","    d^2 u      du\n","    ----- + mu -- + ku = 0\n","    dt^2       dt\n","\n","    Boundary conditions:\n","    u (0) = 1\n","    u'(0) = 0\n","    \"\"\"\n","\n","    assert t.shape == (1,)\n","\n","    def u_fn(t):\n","        \"\"\"Calls network.forward, isolating u as a scalar function of a scalar input\n","        suitable for jax.grad\"\"\"\n","        return network.forward(parameters, t.reshape(1,)).squeeze()# has shape ()\n","    t = t.squeeze()# has shape ()\n","\n","    ## Task: similar to `physics_loss`, compute u and dudt, and evaluate the boundary losses\n","\n","\n","\n","    ##\n","\n","    assert loss_displacement.shape == loss_velocity.shape == ()\n","    return loss_displacement, loss_velocity\n","\n","\n","def PINN_loss_batch(parameters, t_boundary, t_physics_batch, network, mu, k):\n","    \"\"\"Computes the total PINN loss for the harmonic oscillator problem, across a batch of collocation points.\n","    Parameters:\n","        parameters: network parameters\n","        t_boundary: SINGLE input point of shape (1,)\n","        t_physics_batch: BATCH of collocation points of shape (N, 1)\n","        network: network class\n","        mu: coefficient of friction\n","        k: spring constant\n","    Returns:\n","        loss: SINGLE SCALAR loss value of shape ()\n","    \"\"\"\n","\n","    assert t_boundary.shape == (1,) and t_physics_batch.ndim == 2\n","\n","    ## Task: use `jax.vmap` to evaluate the physics loss over the batch of collocation points\n","    ## t_physics_batch, and compute the mean loss value across the batch.\n","    ## also evaluate the boundary losses for the single t_boundary point.\n","\n","\n","\n","    ##\n","\n","    assert loss_physics.shape == loss_displacement.shape == loss_velocity.shape == ()\n","\n","    # sum all the losses together, weighting terms appropriately\n","    loss = 1e-4*loss_physics + loss_displacement + 1e-1*loss_velocity\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"FKM3kVDcVfX1"},"source":["### Step 3: Training the PINN and evaluating performance\n","\n","Finally, we need to train the PINN by optimising the PINN's loss function. Rather than write our own optimiser, in this example we will use the `adam` optimiser from the `optax` library.\n","\n","> Task 4: train the PINN using the training code provided."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEQ60SI6WSf_"},"outputs":[],"source":["@partial(jax.jit, static_argnums=(1,5))# JAX best practice is to only `jit` your highest-level function\n","def PINN_step(opt_state, optimiser, parameters, t_boundary, t_physics_batch, network, mu, k):\n","    \"Updates PINN parameters using the `optax` Adam optimiser\"\n","\n","    # get loss and gradient over batch\n","    loss, grads = jax.value_and_grad(PINN_loss_batch, argnums=0)(\n","        parameters, t_boundary, t_physics_batch, network, mu, k)\n","\n","    # apply parameter update\n","    updates, opt_state = optimiser.update(grads, opt_state, parameters)\n","    parameters = optax.apply_updates(parameters, updates)\n","\n","    return loss, opt_state, parameters\n","\n","\n","# Train the PINN\n","\n","# define boundary point, for the boundary loss\n","t_boundary = jnp.array([0.])# has shape (1,)\n","\n","# define batch of training points over the entire problem domain [0,1], for the physics loss\n","t_physics_batch = jnp.linspace(0,1,50).reshape(-1,1)# has shape (50, 1)\n","\n","# define ODE parameters\n","d, w0 = 2, 20\n","mu, k = 2*d, w0**2\n","\n","# get dense test points and exact solution to compare to\n","t_test_batch = jnp.linspace(0,1,300).reshape(-1,1)\n","u_exact_batch = jax.vmap(exact_solution, in_axes=(None,None,0))(d, w0, t_test_batch)\n","\n","# define the optimiser\n","optimiser = optax.adam(learning_rate=1e-3)\n","opt_state = optimiser.init(parameters)\n","\n","# start training\n","start = time.time()\n","for i in range(15000):\n","\n","    # update parameters\n","    loss, opt_state, parameters = PINN_step(\n","        opt_state, optimiser, parameters, t_boundary, t_physics_batch, network, mu, k)\n","\n","    # plot the result as training progresses\n","    if (i+1)%5000 == 0 or i==0:\n","\n","        # get PINN prediction and plot\n","        u_test_batch = jax.vmap(network.forward, in_axes=(None,0))(parameters, t_test_batch)\n","        plot_result(i, t_physics_batch, t_boundary, t_test_batch, u_exact_batch, u_test_batch)\n","\n","print(f\"Total training time: {time.time()-start} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"gFngo3rFVffh"},"source":["> Task 5: measure the total training time, and compare this to the PyTorch implementation. Which is faster, PyTorch or JAX? Why?\n","\n","> Task 6: measure the total training time without `jit` compilation - what's the difference?\n","\n","> Task 7: increase the solution frequency, e.g. by setting `w0 = 40`, can you still train the PINN?\n","\n","> Optional extension tasks. If you have the time, consider these extensions:\n","> 1. Can you optimise training speed and convergence rate any further (e.g. by changing the loss weighting, or network architecture?)\n","> 2. Solve an inverse problem with the PINN: add a supervised observational data loss to `PINN_loss_batch` and try to learn one of the ODE coefficients (e.g. mu) by including mu inside `parameters`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUBWTVUYWUZt"},"outputs":[],"source":["## Task: measure the total training time, and compare this to the PyTorch implementation.\n","## Which is faster, PyTorch or JAX? Why?\n","\n","## Task: measure the total training time without `jit` compilation - what's the difference?\n","\n","## Task: increase the solution frequency, e.g. by setting `w0 = 40`, can you still train the PINN?\n","\n","## Task (optional extensions): if you have the time, consider these extensions:\n","    # 1. Can you optimise training speed and convergence rate any further (e.g. by changing the loss weighting, or network architecture?)\n","    # 2. Solve an inverse problem with the PINN: add a supervised observational data loss to `PINN_loss_batch`\n","    # and try to learn one of the ODE coefficients (e.g. mu) by including mu inside `parameters`"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}