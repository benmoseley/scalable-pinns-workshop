{"cells":[{"cell_type":"markdown","metadata":{"id":"4RZbVAreVGsG"},"source":["# Session 3 - Practical: Coding finite basis PINNs and extreme learning machine FBPINNs in JAX\n","\n","## Goal\n","\n","The goal of this practical is to:\n","- Understand how to use **domain decomposition** to improve the **efficiency and accuracy** of physics-informed neural networks (PINNs)\n","- Understand how it is possible to further accelerate training by **linearising** the model and using a **least-squares** solver to train it.\n","\n","## Contents\n","\n","This practical has two parts:\n","- Part 1: Training a finite basis PINN (**FBPINN**) to solve the 1D harmonic oscillator\n","- Part 2: Using **extreme learning learning machines** and **numerical linear algebra** to further accelerate training.\n","\n","Please see the lecture slides from Session 3 for more explanation on these topics."]},{"cell_type":"markdown","metadata":{"id":"U5Yi6AaeeXlT"},"source":["## Part 1: Training a finite basis PINN (**FBPINN**) to solve the 1D harmonic oscillator\n","\n","<div style=\"text-align: center; margin-top: 20px\">\n","    <img src=\"https://raw.githubusercontent.com/benmoseley/FBPINNs/main/images/workflow.png\" width=800px>\n","</div>\n","<div style=\"text-align: center; font-weight: bold; margin-top: 5px; margin-bottom: 20px\">\n","    Fig 1: FBPINN workflow overview\n","</div>\n","\n","### How are FBPINNs different from PINNs?\n","\n","As discussed in the lecture, to improve the scalability of PINNs to high frequency/ multiscale solutions:\n","\n","- FBPINNs divide the problem domain into many small, **overlapping subdomains** (Fig 1).\n","\n","- A neural network is placed within each subdomain, and the solution to the PDE is defined as the **summation over all subdomain networks**.\n","\n","- Each subdomain network is **locally confined** to its subdomain by multiplying it by a smooth, differentiable window function.\n","\n","- Finally, the inputs of each network are **individually normalised** over their subdomain.\n","\n","The hypothesis is that this \"divide and conquer\" approach significantly reduces the complexity of the PINN optimisation problem. Furthermore, individual subdomain normalisation ensures the \"effective\" frequency each subdomain network sees is low, reducing the effect of spectral bias.\n","\n","<div style=\"text-align: center; margin-top: 20px\">\n","    <img src=\"images/fbpinn.png\" alt=\"FBPINN workflow overview\" width=500px>\n","</div>\n","<div style=\"text-align: center; font-weight: bold; margin-top: 5px; margin-bottom: 20px\">\n","    Fig 2: Example FBPINN individual subdomain solutions, window functions, and subdomains, for the 1D harmonic oscillator problem\n","</div>\n","\n","Note that FBPINNs can simply be thought of as defining a **custom neural network architecture** for PINNs. We train PINNs using **exactly the same loss function** as before.\n","\n","For the 1D harmonic oscillator problem, we write the equation for the FBPINN architecture as:\n","$$\n","\\hat{u}(t,\\mathbf{\\theta})=  \\sum_{j = 1}^J \\omega_j(t) NN_j((t-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_j)~,\n","$$\n","where $NN_j$ are the individual subdomain networks with learnable parameters $\\mathbf{\\theta}_j$, $\\omega_j$ are window functions constraining each subdomain network to its subdomain, $\\mu_j$ and $\\sigma_j$ are the center and half-width of each subdomain, and $J$ is the total number of subdomains.\n","\n","To ensure stability during training, we also impose that the window functions form a **partition of unity**. This is a fancy way of saying that they should always sum to 1, i.e. $\\sum_{j = 1}^J  \\omega_j(t)=1 \\ \\ \\forall t$. An easy way of doing this in practice is picking some unnormalised window function, $\\hat{\\omega}_j$, (say for example $\\hat{\\omega}_j= Gaussian(\\mu_j, \\sigma_j)$), and then defining\n","$$\n","\\omega_j(t) := \\frac{\\hat{\\omega}_j(t)}{\\sum_{i = 1}^J  \\hat{\\omega}_i(t)}~.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovPQ3iHiZFkW"},"outputs":[],"source":["import time\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","\n","import jax\n","import jax.numpy as jnp\n","import optax\n","\n","def exact_solution(d, w0, t):\n","    \"Computes the analytical solution to the under-damped harmonic oscillator.\"\n","    assert d < w0\n","    w = jnp.sqrt(w0**2-d**2)\n","    phi = jnp.arctan(-d/w)\n","    A = 1/(2*jnp.cos(phi))\n","    cos = jnp.cos(phi+w*t)\n","    exp = jnp.exp(-d*t)\n","    u = exp*2*A*cos\n","    return u\n","\n","def plot_result(i, t_physics_batch, t_boundary, t_test_batch, u_exact_batch, u_test_batch):\n","    \"Plots the PINN's prediction\"\n","    test_error = jnp.mean(jnp.abs(u_test_batch-u_exact_batch)/u_exact_batch.std())\n","    plt.figure(figsize=(8,3))\n","    plt.scatter(t_physics_batch[:,0], jnp.zeros_like(t_physics_batch)[:,0], s=20, lw=0, color=\"tab:blue\", alpha=0.6, label=\"Collocation points\")\n","    plt.scatter(t_boundary, 0, s=20, lw=0, color=\"tab:red\", alpha=0.6, label=\"Boundary point\")\n","    plt.plot(t_test_batch[:,0], u_exact_batch[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n","    plt.plot(t_test_batch[:,0], u_test_batch[:,0], label=\"PINN solution\", color=\"tab:green\")\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.title(f\"Training step {i+1}    Relative L1 test error: {test_error:.2%}\")\n","    plt.legend()\n","    plt.show()\n","\n","class FCN:\n","    \"\"\"Fully connected network in JAX, using only jax.numpy\n","\n","    Note for the curious reader: any functions which are designed for JAX transformations (vmap, grad, jit, etc)\n","    should not include side-effects (a side-effect is any effect of a function that doesnâ€™t appear in its output).\n","\n","    But standard python class methods (e.g. def __init__(self, ..)) often adjust `self` outside of the method.\n","    This is risky, unless you know exactly what you are doing; and is why we only define static methods here.\n","\n","    This means the FCN class is really just a collection of functions, i.e. a convienent namespace,\n","    and we carry around the state of the network (i.e. its `parameters`, normally contained in `self` in PyTorch)\n","    explicitly in our JAX code, passing it explicitly to each method. See here for more discussion:\n","    https://docs.jax.dev/en/latest/stateful-computations.html\n","    \"\"\"\n","\n","    @staticmethod\n","    def init_parameters(key, layer_sizes):\n","        \"\"\"Initialise the parameters of the network.\n","        Parameters:\n","            key: current JAX RNG state\n","            layer_sizes: list defining the number of layers and the number of channels per layer, including\n","            input/output layers, e.g. [1,16,16,1]\n","        Returns:\n","            parameters: list of randomly initialised weights and biases [(W0, b0), ...]\n","\n","        Note: JAX uses explicit random seed management; so we need to pass the current RNG state (`key`) explicitly\n","        to any `jax.random` calls, and split the RNG state by hand.\n","        \"\"\"\n","\n","        keys = jax.random.split(key, len(layer_sizes)-1)# split the key\n","        parameters = [FCN._random_layer_parameters(k, m, n)\n","                      for k, m, n in zip(keys, layer_sizes[:-1], layer_sizes[1:])]# initialise all layers\n","\n","        return parameters\n","\n","    @staticmethod\n","    def _random_layer_parameters(key, m, n):\n","        \"Randomly intialise the parameters of a single layer\"\n","\n","        W_key, b_key = jax.random.split(key)\n","        v = jnp.sqrt(1/m)# LeCun initialisation\n","        W = jax.random.uniform(W_key, (n, m), minval=-v, maxval=v)\n","        b = jax.random.uniform(b_key, (n,), minval=-v, maxval=v)\n","\n","        return W,b\n","\n","    @staticmethod\n","    def forward(parameters, x):\n","        \"\"\"Forward pass of the network.\n","        Parameters:\n","            parameters: list of weights and biases [(W0,b0), ...]\n","            x: SINGLE input point of shape (xdim=layer_sizes[0],)\n","        Returns:\n","            u: SINGLE output point of shape (udim=layer_sizes[-1],)\n","\n","        Note: a key philosophical difference between JAX and PyTorch is that in JAX we define non-batched\n","        operations (like the forward pass of the network with a SINGLE input point), and then use `vmap`\n","        afterwards to define the batched version (e.g. for evaluating the network over many points).\n","        With experience, this is a powerful way to build sophisticated, peformant, flexible code which remains\n","        readable and is less bug-prone.\n","        \"\"\"\n","\n","        assert x.ndim == 1\n","        activation_fn = jnp.tanh\n","\n","        for W, b in parameters[:-1]:# W has shape (m, n), b has shape (n,)\n","            x = jnp.dot(W, x) + b\n","            x = activation_fn(x)\n","        W, b = parameters[-1]\n","        u = jnp.dot(W, x) + b\n","\n","        assert u.ndim == 1\n","        return u"]},{"cell_type":"markdown","metadata":{"id":"Oq7xdvHIZ8xp"},"source":["### Step 1: Defining the FBPINN model JAX\n","\n","The first step is to define this FBPINN architecture in JAX. To do this, we define the `FBPINN` class, which initialises all of the subdomain network parameters, defines the subdomain ranges and window functions, and computes the individual outputs of each subdomain. The `FBPINN.forward` method then computes the final output of the FBPINN: the summation over all subdomains network outputs.\n","\n","> Task 1: complete the `FBPINN.init_parameters`, `FBPINN.forward_single_subdomain`, and `FBPINN.forward` functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QD8jwEaAW4eY"},"outputs":[],"source":["class FBPINN:\n","    \"Finite basis physics-informed neural network\"\n","\n","    @staticmethod\n","    def init_parameters(key, layer_sizes, J):\n","        \"\"\"Initialise the parameters for all subdomain networks inside the FBPINN.\n","        Parameters:\n","            key: current JAX RNG state\n","            layer_sizes: list defining the number of layers and the number of channels per layer, including\n","            input/output layers, e.g. [1,16,16,1], which each subdomain network uses\n","            J: total number of subdomains\n","        Returns:\n","            parameters: list of randomly initialised weights and biases [(W0, b0), ...] with a leading subdomain dimension.\n","            i.e. W0 now has shape (J, m, n), and b has shape (J, n)\n","        \"\"\"\n","\n","        ## Task: initialise the parameters for all of the subdomain networks. Each subdomain network\n","        ## should use the same FCN architecture with the same `layer_sizes`.\n","        ## Hint: re-use FCN.init_params, and `vmap` it to generate all parameters at once.\n","\n","\n","\n","        ##\n","\n","        for W,b in parameters: assert W.ndim == 3 and b.ndim == 2 and W.shape[0] == b.shape[0] == J\n","        return parameters\n","\n","    @staticmethod\n","    def get_subdomain_tranges(J, width):\n","        \"\"\"Gets the tmin, tmax range of each subdomain, assuming a [0,1] problem domain, and a regular domain decomposition.\n","        Parameters:\n","            J: total number of subdomains\n","            width: width of each subdomain\n","        Returns:\n","            tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","        \"\"\"\n","\n","        assert width > 1/(J-1), f\"width is too small: {J} subdomains of width {width} will not fill entire [0,1] interval\"\n","\n","        if J == 1:\n","            t_centers = jnp.array([0.5])\n","        else:\n","            t_centers = jnp.linspace(0,1,J)\n","        tmins = t_centers - width/2\n","        tmaxs = t_centers + width/2\n","        tranges = jnp.stack([tmins, tmaxs], axis=1)# (J, 2)\n","\n","        assert tranges.shape == (J, 2)\n","        return tranges\n","\n","    @staticmethod\n","    def window(trange, t):\n","        \"\"\"Defines the window function used by the FBPINN, in this case a cosine window function,\n","        for a SINGLE input point of shape (1,)\n","        \"\"\"\n","\n","        assert t.shape == (1,) and trange.shape == (2,)\n","        tmin, tmax = trange\n","        mu, sd = (tmin+tmax)/2, (tmax-tmin)/2\n","        w = ((1+jnp.cos(jnp.pi*(t-mu)/sd))/2)**2\n","        w = jnp.heaviside(t-tmin,1)*jnp.heaviside(tmax-t,1)*w\n","\n","        return w\n","\n","    @staticmethod\n","    def forward_single_subdomain(parameters_for_a_single_subdomain, trange, t):\n","        \"\"\"Computes the output of a single subdomain in the FBPINN.\n","        Parameters:\n","            parameters_for_a_single_subdomain: list of FCN weights and biases [(W0,b0), ...]\n","            for a single subdomain (no leading subdomain dimension)\n","            trange: array of shape (2,) containing the tmin,tmax of the subdomain\n","            t: a SINGLE input point of shape (1,)\n","        Returns:\n","            u: a SINGLE solution point at t, of shape (1,)\n","            w: the value of the subdomain's window function at t, of shape (1,)\n","\n","        This function should compute u_j(t) = window_j(t) * NN_j(norm_j(t)) in the FBPINN ansatz.\n","        \"\"\"\n","\n","        assert t.shape == (1,) and trange.shape == (2,)\n","        tmin, tmax = trange\n","        mu, sd = (tmin+tmax)/2, (tmax-tmin)/2\n","\n","        ## Task: finish evaluating the forward function for a single subdomain network in the FBPINN\n","        ## Don't forget to normalise the input of the network within the subdomain range\n","        ## and apply the window function to the output of the network\n","\n","\n","\n","        ##\n","\n","        # `u` should be the subdomain output, u_j(t), (with the window applied), `w` should be the window value, window_j(t)\n","        assert u.shape == w.shape == (1,)\n","        return u, w\n","\n","    @staticmethod\n","    def forward(parameters, tranges, t):\n","        \"\"\"Computes the FBPINN solution.\n","        Parameters:\n","            parameters: list of FCN weights and biases [(W0,b0), ...] with leading subdomain dimension\n","            tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","            t: a SINGLE input point of shape (1,)\n","        Returns:\n","            u: a SINGLE solution point of shape (1,)\n","\n","        This function should compute the full FBPINN solution, u(t) = sum_j (1/window_norm(t)) u_j(t), where\n","        window_norm(t) = 1 / (sum_j window_j(t)) is used to ensure a partition of unity (i.e. all window functions sum to 1).\n","        \"\"\"\n","\n","        assert t.shape == (1,) and tranges.ndim == 2 and tranges.shape[1] == 2\n","\n","        ## Task: finish defining the forward function.\n","        ## Hint: `vmap` forward_single_subdomain, and sum the subdomain solutions together\n","        ## don't forget to apply the partition of unity (window normalisation) after summing the subdomain solutions\n","\n","\n","\n","        ##\n","\n","        assert u.shape == (1,)\n","        return u"]},{"cell_type":"markdown","metadata":{"id":"R47EaSFmVi03"},"source":["> Task 2: plot the domain decomposition, window functions, and the individual FBPINN subdomain solutions for the untrained FBPINN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c77BOZy6W8bP"},"outputs":[],"source":["# set random seed\n","key = jax.random.PRNGKey(0)\n","\n","# define the neural network architecture and initialise its parameters\n","network = FBPINN\n","J = 4\n","parameters = network.init_parameters(key, [1,8,1], J)\n","tranges = network.get_subdomain_tranges(J, 2/(J-1))\n","\n","# define ODE parameters\n","d, w0 = 2, 20\n","mu, k = 2*d, w0**2\n","\n","# get dense test points and exact solution to compare to\n","t_test_batch = jnp.linspace(0,1,300).reshape(-1,1)\n","u_exact_batch = jax.vmap(exact_solution, in_axes=(None,None,0))(d, w0, t_test_batch)\n","\n","\n","## Task: plot the domain decomposition, window functions, and the individual FBPINN subdomain solutions of the untrained FBPINN\n","## hint: you can `vmap` `FBPINN.forward_single_subdomain` to get all individual solutions and their window functions,\n","## and the domain decomposition is defined by `tranges`. Don't forget to apply the partition of unity to the individual\n","# subdomain solutions and window functions after you `vmap` `FBPINN.forward_single_subdomain`\n","\n","\n","\n","##"]},{"cell_type":"markdown","metadata":{"id":"YoWT6CcPVi3z"},"source":["### Step 2: Computing the PINN loss function\n","\n","As mentioned above, FBPINNs are trained using **exactly the same loss function** as PINNs. So we can simply reuse all of the loss functions we wrote for the previous PINN practical.\n","\n","> Task 3: simply run the cell below to redefine the PINN loss functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASkU3bxpXBHh"},"outputs":[],"source":["def FBPINN_physics_loss(parameters, tranges, t, network, mu, k):\n","    \"Compute the physics loss for the 1D harmonic oscillator problem\"\n","\n","    assert t.shape == (1,)\n","\n","    def u_fn(t):\n","        \"\"\"Calls network.forward, isolating u as a scalar function of a scalar input\n","        suitable for jax.grad\"\"\"\n","        return network.forward(parameters, tranges, t.reshape(1,)).squeeze()# has shape ()\n","    t = t.squeeze()# has shape ()\n","\n","    u = u_fn(t)# has shape ()\n","    dudt_fn = jax.grad(u_fn)\n","    dudt = dudt_fn(t)\n","\n","    d2udt2_fn = jax.grad(dudt_fn)\n","    d2udt2 = d2udt2_fn(t)\n","\n","    loss_physics = (d2udt2 + mu*dudt + k*u)**2\n","\n","    assert loss_physics.shape == ()\n","    return loss_physics\n","\n","\n","def FBPINN_boundary_losses(parameters, tranges, t, network):\n","    \"Compute the two boundary losses for the 1D harmonic oscillator problem\"\n","\n","    assert t.shape == (1,)\n","\n","    def u_fn(t):\n","        \"\"\"Calls network.forward, isolating u as a scalar function of a scalar input\n","        suitable for jax.grad\"\"\"\n","        return network.forward(parameters, tranges, t.reshape(1,)).squeeze()# has shape ()\n","    t = t.squeeze()# has shape ()\n","\n","    u = u_fn(t)\n","    dudt_fn = jax.grad(u_fn)\n","    dudt = dudt_fn(t)\n","    loss_displacement, loss_velocity = (u-1)**2, (dudt-0)**2\n","\n","    assert loss_displacement.shape == loss_velocity.shape == ()\n","    return loss_displacement, loss_velocity\n","\n","\n","def FBPINN_loss_batch(parameters, tranges, t_boundary, t_physics_batch, network, mu, k):\n","    \"Computes the total FBPINN loss for the harmonic oscillator problem, across a batch of collocation points\"\n","\n","    assert t_boundary.shape == (1,) and t_physics_batch.ndim == 2\n","\n","    loss_physics = jnp.mean(jax.vmap(FBPINN_physics_loss, in_axes=(None, None, 0, None, None, None))(\n","        parameters, tranges, t_physics_batch, network, mu, k))\n","\n","    loss_displacement, loss_velocity = FBPINN_boundary_losses(parameters, tranges, t_boundary, network)\n","\n","    assert loss_physics.shape == loss_displacement.shape == loss_velocity.shape == ()\n","\n","    # sum all the losses together, weighting terms appropriately\n","    loss = 1e-4*loss_physics + loss_displacement + 1e-1*loss_velocity\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"vQpLTReqVi7C"},"source":["### Step 3: Training the FBPINN and evaluating performance\n","\n","The final step is to train the FBPINN. We will use the same problem setup and training code as the PINN practical.\n","\n","> Task 4: train the FBPINN using the training code provided."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bI5iAAmQXF9F"},"outputs":[],"source":["@partial(jax.jit, static_argnums=(1,6))# JAX best practice is to only `jit` your highest-level function\n","def FBPINN_step(opt_state, optimiser, parameters, tranges, t_boundary, t_physics_batch, network, mu, k):\n","    \"Updates FBPINN parameters using the `optax` Adam optimiser\"\n","\n","    # get loss and gradient over batch\n","    loss, grads = jax.value_and_grad(FBPINN_loss_batch, argnums=0)(\n","        parameters, tranges, t_boundary, t_physics_batch, network, mu, k)\n","\n","    # apply parameter update\n","    updates, opt_state = optimiser.update(grads, opt_state, parameters)\n","    parameters = optax.apply_updates(parameters, updates)\n","\n","    return loss, opt_state, parameters\n","\n","\n","\n","# Train the FBPINN\n","\n","# define boundary point, for the boundary loss\n","t_boundary = jnp.array([0.])# has shape (1,)\n","\n","# define batch of training points over the entire problem domain [0,1], for the physics loss\n","t_physics_batch = jnp.linspace(0,1,50).reshape(-1,1)# has shape (50, 1)\n","\n","# define the optimiser\n","optimiser = optax.adam(learning_rate=1e-3)\n","opt_state = optimiser.init(parameters)\n","\n","# start training\n","start = time.time()\n","for i in range(15000):\n","\n","    # update parameters\n","    loss, opt_state, parameters = FBPINN_step(\n","        opt_state, optimiser, parameters, tranges, t_boundary, t_physics_batch, network, mu, k)\n","\n","    # plot the result as training progresses\n","    if (i+1)%5000 == 0 or i==0:\n","\n","        # get PINN prediction and plot\n","        u_test_batch = jax.vmap(network.forward, in_axes=(None,None,0))(parameters, tranges, t_test_batch)\n","        plot_result(i, t_physics_batch, t_boundary, t_test_batch, u_exact_batch, u_test_batch)\n","\n","print(f\"Total training time: {time.time()-start} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"bWX0ew-MVi91"},"source":["> Task 5: measure the total training time, and compare this to the JAX PINN. Which is faster, and why?\n","\n","> Task 6: increase the solution frequency, e.g. by setting `w0 = 40`, can you beat the PINN's performance with the FBPINN?\n","\n","> Optional extension tasks. If you have the time, consider these extensions:\n","> 1. Can you optimise training speed and convergence rate any further (e.g. by increasing the number of subdomains, size of the subdomain networks, or amount of overlap between the subdomains)?\n","> 2. Plot test accuracy vs solution frequency: can the FBPINN scale to very high frequencies?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omIB5PROXJpt"},"outputs":[],"source":["## Task: measure the total training time, and compare this to the JAX PINN. Which is faster, and why?\n","\n","## Task: increase the solution frequency, e.g. by setting `w0 = 40`, can you beat the PINN's performance with the FBPINN?\n","\n","## Task (optional extensions): if you have the time, consider these extensions:\n","    # 1. Can you optimise training speed and convergence rate any further (e.g. by increasing the\n","    # number of subdomains, size of the subdomain networks, or amount of overlap between the subdomains)?\n","    # 2. Plot test accuracy vs solution frequency: can the FBPINN scale to very high frequencies?"]},{"cell_type":"markdown","metadata":{"id":"J7IjQ91rVjAa"},"source":["## Part 2: Using **extreme learning learning machines** and **numerical linear algebra** to further accelerate training\n","\n","### How can we further accelerate FBPINNs by linearising the model?\n","\n","As discussed in the lecture, the major downside to FBPINNs is that their loss function is still **highly nonlinear**, which means they require a nonlinear optimisation algorithm to train them (and in practice, most often, we use gradient descent).\n","\n","These types of optimisation algorithms often add a significant **computational overhead** - for example, when using gradient descent, computing derivatives of the FBPINN solution and loss function at each gradient descent step is very expensive (even with the power of JAX!).\n","\n","To overcome this limitation, we are going to **linearise** the FBPINN model with respect to its trainable parameters. For **linear ODEs and PDEs**, this will allow us to turn their loss function into a **least-squares** problem which can be solved much more efficiently using **traditional numerical linear algebra techniques**, rather than gradient desent.\n","\n","### Extreme learning machine FBPINNs (aka ELM-FBPINNs)\n","\n","To linearise FBPINNs, we will use **[extreme learning machines](https://www.sciencedirect.com/science/article/pii/S0925231206000385)** as the subdomain networks in the FBPINN. An extreme learning machine is simply a **randomly initialised network, where all of its hidden parameters are fixed and not trained**, and only the **weights of the last layer** are trained.\n","\n","More specifically, we now assume the FBPINN solution can be written down as\n","$$\n","\\hat{u}(t,\\mathbf{a})=  \\sum_{j = 1}^J \\omega_j(t) \\sum_{k = 1}^K a_{jk} \\phi_{jk}((t-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk})~,\n","$$\n","where $\\phi_{jk}(\\cdot,\\mathbf{\\theta}_{jk})$ are the $K$ outputs (features) of the last hidden layer of each subdomain network, where the hidden parameters of each subdomain network $\\mathbf{\\theta}_{jk}$ are randomly initialised and not trained. Only the weights of the last layer of each subdomain network, $a_{jk}$, are treated as trainable parameters.\n","\n","\n","Note that we can shift all summations to the left:\n","$$\n","\\hat{u}(t,\\mathbf{a})=  \\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\ \\omega_j(t) \\phi_{jk}((t-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk})~,\n","$$\n","And it is now clear that this model is **linear** in the trainable coefficients $a_{jk}$.\n","\n","<div style=\"text-align: center; margin-top: 20px\">\n","    <img src=\"images/elm_fbpinn_basis.png\" alt=\"FBPINN workflow overview\" width=500px>\n","</div>\n","<div style=\"text-align: center; font-weight: bold; margin-top: 5px; margin-bottom: 20px\">\n","    Fig 3: Example randomly initialised basis functions in the ELM-FBPINN (with windows applied). In this example J=4 and K=16.\n","</div>\n","\n","### Training ELM-FBPINNs\n","\n","Again, we train ELM-FBPINNs using the same loss function as PINNs and FBPINNs. Recall, for the 1D harmonic oscillator problem, this is given by\n","$$\n","L(\\theta)= (\\hat{u}(0,\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d \\hat{u}}{dt}(0,\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N_I} \\sum^{N_I}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] \\hat{u}(t_{i},\\theta) Â \\right)^2~.\n","$$\n","Substituting in our equation for the ELM-FBPINN solution, and only treating the coefficients $a_{jk}$ as learnable, we get\n","$$\n","\\begin{aligned}\n","L(\\mathbf{a}) &= \\left(\\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\omega_j(t) \\phi_{jk}((0-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk}) - 1\\right)^2 \\\\\n","& + \\lambda_1 \\left(\\frac{d }{dt}\\left[\\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\omega_j(t) \\phi_{jk}((0-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk})\\right] - 0\\right)^2 \\\\\n","&+ \\frac{\\lambda_2}{N_I} \\sum^{N_I}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] \\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\omega_j(t) \\phi_{jk}((t_i-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk}) Â \\right)^2~.\n","\\end{aligned}\n","$$\n","We can see that, **because our ODE and boundary conditions are linear operators, we can shift all summations to the left**:\n","$$\n","\\begin{aligned}\n","L(\\mathbf{a}) &= \\left(\\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\ \\omega_j(t) \\phi_{jk}((0-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk}) - 1\\right)^2 \\\\\n","& + \\lambda_1 \\left(\\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\ \\frac{d }{dt}\\left[\\omega_j(t) \\phi_{jk}((0-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk})\\right] - 0\\right)^2 \\\\\n","&+ \\frac{\\lambda_2}{N_I} \\sum^{N_I}_{i} \\left(  \\sum_{j = 1}^J \\sum_{k = 1}^K a_{jk} \\ \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] \\omega_j(t) \\phi_{jk}((t_i-\\mu_j)/\\sigma_j, \\mathbf{\\theta}_{jk}) - 0Â \\right)^2~.\n","\\end{aligned}\n","$$\n","This allows us to re-write minimising the loss as the following **least-squares problem**:\n","$$\n","\\min_{\\mathbf{a}}  \\left\\| \\mathbf{B}_1 \\mathbf{a} - \\mathbf{g}_1 \\right\\|^2  + \\lambda_1 \\left\\| \\mathbf{B}_2 \\mathbf{a} - \\mathbf{g}_2 \\right\\|^2 + \\frac{\\lambda_2}{N_I} \\left\\| \\mathbf{N} \\mathbf{a} - \\mathbf{f} \\right\\|^2~,\n","$$\n","Where the matrices $\\mathbf{B}_1 \\in \\mathbb{R}^{1 \\times JK}$, $\\mathbf{B}_2 \\in \\mathbb{R}^{1 \\times JK}$, and $\\mathbf{N} \\in \\mathbb{R}^{N_I \\times JK}$ contain evaluations of the boundary/ODE operators on the windowed basis functions at boundary/collocation points, and the vectors $\\mathbf{g}_1 = \\mathbf{1} \\in \\mathbb{R}^{1}$, $\\mathbf{g}_2 = \\mathbf{0} \\in \\mathbb{R}^{1}$, and $\\mathbf{f} = \\mathbf{0} \\in \\mathbb{R}^{N_I}$ contain evaluations of the boundary/ODE forcing functions at boundary/collocation points.\n","\n","Note, the least-squares problem can be equivalently written as a single combined least-squares problem\n","$$\n","\\min_{\\mathbf{a}} \\left\\| \\mathbf{M} \\mathbf{a} - \\mathbf{h} \\right\\|^2~,\n","$$\n","by vertically stacking the matrices and vectors,\n","$$\n","\\mathbf{M} := \\begin{bmatrix} \\sqrt{\\frac{\\lambda_2}{N_I}} \\mathbf{N} \\\\ \\mathbf{B}_1 \\\\ \\sqrt{\\lambda_1} \\mathbf{B}_2 \\end{bmatrix}, \\quad \\mathbf{h} := \\begin{bmatrix} \\sqrt{\\frac{\\lambda_2}{N_I}} \\mathbf{f} \\\\ \\mathbf{g}_1 \\\\ \\sqrt{\\lambda_1} \\mathbf{g}_2 \\end{bmatrix}~.\n","$$\n","\n","We are done! All we need to do now is assemble the least-squares system, and then use a traditional **least-squares solver** (such as `jax.numpy.linalg.lstsq`) to find $a_{jk}$, avoiding the need for gradient descent!"]},{"cell_type":"markdown","metadata":{"id":"kNEH_T6zep3_"},"source":["### Step 1: Defining the ELM-FBPINN model in JAX\n","\n","Our first step is to rewrite the `FBPINN` class so that it uses ELMs as its subdomain networks. We also need to be able to compute and access the outputs of the $J\\times K$ windowed basis functions, $\\omega_j \\phi_{jk}$.\n","\n","> Task 1: complete the `ELMFBPINN.forward` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrRlE1XMXN_U"},"outputs":[],"source":["class ELMFBPINN(FBPINN):\n","    \"Extreme learning machine FBPINN\"\n","\n","    @staticmethod\n","    def forward_basis_single_subdomain(parameters_for_a_single_subdomain, trange, t):\n","        \"\"\"Computes the (unnormalised) windowed basis functions of a single subdomain in the ELM-FBPINN.\n","        Parameters:\n","            parameters_for_a_single_subdomain: list of FCN weights and biases [(W0,b0), ...]\n","            for a single subdomain (no leading subdomain dimension)\n","            trange: array of shape (2,) containing the tmin,tmax of the subdomain\n","            t: a SINGLE input point of shape (1,)\n","        Returns:\n","            phi: array of windowed basis function values at t, of shape (K,)\n","            w: the value of the subdomain's window function at t, of shape (1,)\n","\n","        This function computes window_j(t) * phi_jk (norm_j(t)) for all k\n","        i.e. the unnormalised windowed basis functions for a single subdomain\n","        \"\"\"\n","\n","        assert t.shape == (1,) and trange.shape == (2,)\n","        tmin, tmax = trange\n","        mu, sd = (tmin+tmax)/2, (tmax-tmin)/2\n","\n","        w = FBPINN.window(trange, t)\n","        t = (t-mu)/sd\n","        phi = FCN.forward(parameters_for_a_single_subdomain, t)\n","        phi = jnp.tanh(phi)# apply a tanh activation to output of FCN\n","        phi = w*phi# phi has shape (K,) and w has shape (1,)\n","\n","        assert w.shape == (1,) and phi.ndim == 1\n","        return phi, w\n","\n","    @staticmethod\n","    def forward_basis(parameters_fixed, tranges, t):\n","        \"\"\"Computes the windowed basis functions for all subdomains in the ELM-FBPINN.\n","        Parameters:\n","            parameters_fixed: list of FCN weights and biases [(W0,b0), ...] with leading subdomain dimension\n","            tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","            t: a SINGLE input point of shape (1,)\n","        Returns:\n","            phi: array of windowed basis function values at t, of shape (J,K), with POU applied\n","\n","        This function computes (1/window_norm(t)) * window_j(t) * phi_jk (norm_j(t)) for all j and k\n","        i.e. the normalised windowed basis functions for all subdomains\n","        \"\"\"\n","\n","        assert t.shape == (1,)\n","        phis, ws = jax.vmap(ELMFBPINN.forward_basis_single_subdomain, in_axes=(0, 0, None))(\n","            parameters_fixed, tranges, t)# phis has shape (J,K), ws have shape (J,1)\n","        phis = phis/ws.sum()# apply POU, has shape (J,K)\n","        assert phis.ndim == 2\n","        return phis\n","\n","\n","    @staticmethod\n","    def forward(parameters_fixed, tranges, a, t):\n","        \"\"\"Computes the ELM-FBPINN solution.\n","        Parameters:\n","            parameters_fixed: list of FCN weights and biases [(W0,b0), ...] with leading subdomain dimension\n","            tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","            a: basis function coefficients of shape (J, K)\n","            t: a SINGLE input point of shape (1,)\n","        Returns:\n","            u: a SINGLE solution point of shape (1,)\n","\n","        This function should compute the full ELM-FBPINN solution,\n","        u(t) = sum_j sum_k [ a_jk * (1/window_norm(t)) * window_j(t) * phi_jk (norm_j(t)) ]\n","\n","        Note `ELMBPINN.forward_basis` already computes (1/window_norm(t)) * window_j(t) * phi_jk (norm_j(t)), so this\n","        function should just apply the multiplication by a_jk and summation over j and k.\n","        \"\"\"\n","\n","        assert t.shape == (1,) and tranges.ndim == 2 and tranges.shape[1] == 2 and a.ndim == 2\n","\n","        ## Task: make sure you understand the methods defined above, and then finish defining this function\n","        ## Hint: simply call `forward_basis` to get the basis functions of shape (J,K)\n","        ## then multiply these by the coefficients `a` and sum over J and K.\n","\n","\n","\n","        ##\n","\n","        assert u.shape == (1,)\n","        return u"]},{"cell_type":"markdown","metadata":{"id":"XCahJe4_VjCn"},"source":["> Task 2: plot the basis functions for each subdomain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrJeeoqvXO3g"},"outputs":[],"source":["# set random seed\n","key = jax.random.PRNGKey(0)\n","\n","# define the neural network architecture and initialise its parameters\n","network = ELMFBPINN\n","J = 4\n","K = 16\n","parameters_fixed = network.init_parameters(key, [1, K], J)\n","tranges = network.get_subdomain_tranges(J, 2/(J-1))\n","\n","# define ODE parameters\n","d, w0 = 2, 20\n","mu, k = 2*d, w0**2\n","\n","# get dense test points and exact solution to compare to\n","t_test_batch = jnp.linspace(0,1,300).reshape(-1,1)\n","u_exact_batch = jax.vmap(exact_solution, in_axes=(None,None,0))(d, w0, t_test_batch)\n","\n","\n","## Task: plot the basis functions for each subdomain\n","## hint: use `network.forward_basis` to get the basis functions for all subdomain networks\n","## and `vmap` it over a set of test time points\n","\n","\n","\n","##"]},{"cell_type":"markdown","metadata":{"id":"nSkADF5zVjE2"},"source":["### Step 2: Assembling the least-squares system matrices\n","\n","Next we will use the `ELMFBPINN` class to populate the elements of the matrices $\\mathbf{N}$, $\\mathbf{B}_1$, and $\\mathbf{B}_2$. For this, we need to evaluate the boundary/ODE operators on the windowed basis functions, $\\omega_j \\phi_{jk}$, at boundary/collocation points.\n","\n","> Task 3: complete the `physics_terms` and `boundary_terms` functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se_5dKRuXYq2"},"outputs":[],"source":["def physics_terms(parameters_fixed, tranges, t, network, mu, k):\n","    \"\"\"Compute a row of the N matrix and f vector for the 1D harmonic oscillator problem.\n","    Parameters:\n","        parameters_fixed: network parameters\n","        tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","        t: SINGLE input point of shape (1,)\n","        network: network class\n","        mu: coefficient of friction\n","        k: spring constant\n","    Returns:\n","        matrix_N_row: single row of the N matrix, of shape (J*K,)\n","        vector_f_element: single element of the f vector, of shape ()\n","    \"\"\"\n","\n","    assert t.shape == (1,)\n","\n","    def basis_fn(t):\n","        \"\"\"Calls network.forward_basis, isolating phis(t) as a vector function of a scalar input\n","        suitable for jax.jacfwd\"\"\"\n","        return network.forward_basis(parameters_fixed, tranges, t.reshape(1,))# has shape (J,K)\n","    t = t.squeeze()# has shape ()\n","\n","    phis = basis_fn(t)# has shape (J,K)\n","\n","    ## Task: use `jax.jacfwd` to compute the gradients of basis_fn at t, dphisdt and d2phisdt2, which should both have shape (J,K)\n","\n","\n","\n","    ##\n","\n","    assert d2phisdt2.ndim == dphisdt.ndim == phis.ndim == 2\n","\n","    matrix_N_row = d2phisdt2 + mu*dphisdt + k*phis# has shape (J,K)\n","    vector_f_element = 0\n","\n","    return matrix_N_row.flatten(), vector_f_element\n","\n","\n","def boundary_terms(parameters_fixed, tranges, t, network):\n","    \"\"\"Compute a row of the two boundary matrices and vectors for the 1D harmonic oscillator problem.\n","    Parameters:\n","        parameters_fixed: network parameters\n","        tranges: array of shape (J, 2) containing tmin, tmax of each subdomain\n","        t: SINGLE input point of shape (1,)\n","        network: network class\n","        mu: coefficient of friction\n","        k: spring constant\n","    Returns:\n","        matrix_B1_row: single row of the B1 matrix, of shape (J*K,)\n","        vector_g1_element: single element of the g1 vector, of shape ()\n","        matrix_B2_row: single row of the B2 matrix, of shape (J*K,)\n","        vector_g2_element: single element of the g2 vector, of shape ()\n","    \"\"\"\n","\n","    assert t.shape == (1,)\n","\n","    def basis_fn(t):\n","        \"\"\"Calls network.forward_basis, isolating phis(t) as a vector function of a scalar input\n","        suitable for jax.jacfwd\"\"\"\n","        return network.forward_basis(parameters_fixed, tranges, t.reshape(1,))# has shape (J,K)\n","    t = t.squeeze()# has shape ()\n","\n","    ## Task: similar to `physics_terms`, compute phis and dphisdt\n","\n","\n","\n","    ##\n","\n","    assert dphisdt.ndim == phis.ndim == 2\n","\n","    matrix_B1_row = phis# has shape (J,K)\n","    vector_g1_element = 1\n","\n","    matrix_B2_row = dphisdt# has shape (J,K)\n","    vector_g2_element = 0\n","\n","    return matrix_B1_row.flatten(), vector_g1_element, matrix_B2_row.flatten(), vector_g2_element"]},{"cell_type":"markdown","metadata":{"id":"4FujwmfaYUS-"},"source":["> Task 4: run the code below to assemble the M matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVFNDVLCYRmr"},"outputs":[],"source":["def assemble_system(parameters_fixed, tranges, t_physics_batch, t_boundary, network, mu, k):\n","    \"\"\"Assembles M and h in the least-squares system, ||Ma - h||^2 = 0.\n","    Returns:\n","        M: matrix of shape (N, J*K), where N is the total number of training points\n","        and J*K is the total number of basis functions\n","        h: vector of shape (N,)\n","\n","    Note M and h are constructed by vertically stacking the individual physics loss and boundary\n","    losses matrices and vectors.\n","    \"\"\"\n","\n","    # compute matrix entries for physics loss\n","    N, f = jax.vmap(physics_terms, in_axes=(None, None, 0, None, None, None))(\n","        parameters_fixed, tranges, t_physics_batch, network, mu, k)\n","\n","    # compute matrix entries for boundary losses\n","    B1,g1,B2,g2 = jax.vmap(boundary_terms, in_axes=(None, None, 0, None))(\n","        parameters_fixed, tranges, jnp.array([t_boundary]), network)\n","\n","    # get combined least squares system, weighting terms appropriately\n","    M = jnp.concatenate([jnp.sqrt(1e-4/N.shape[0])*N, B1, jnp.sqrt(1e-1)*B2], axis=0)\n","    h = jnp.concatenate([jnp.sqrt(1e-4/N.shape[0])*f, g1, jnp.sqrt(1e-1)*g2], axis=0)\n","\n","    return M, h\n","\n","@partial(jax.jit, static_argnums=(2,3))\n","def solve_system(M, h, J, K):\n","    \"\"\"Solve the least-squares problem ||Ma - h||^2 = 0.\n","    Returns:\n","        a: basis function coefficients of shape (J,K)\n","    \"\"\"\n","\n","    assert M.shape[1] == J*K and M.shape[:1] == h.shape\n","    a, residuals, rank, s = jnp.linalg.lstsq(M, h, rcond=0)\n","    a = a.reshape(J,K)\n","    return a\n","\n","\n","# Train the ELM-FBPINN\n","\n","# define boundary point, for the boundary loss\n","t_boundary = jnp.array([0.])# has shape (1,)\n","\n","# define batch of training points over the entire problem domain [0,1], for the physics loss\n","t_physics_batch = jnp.linspace(0,1,50).reshape(-1,1)# has shape (50, 1)\n","\n","# assemble the linear system\n","M, h = assemble_system(parameters_fixed, tranges, t_physics_batch, t_boundary, network, mu, k)\n","print(f\"Least-squares system assembled: M has shape {M.shape}, h has shape {h.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"lQqDBxSCVjHM"},"source":["> Task 5: plot the sparsity pattern of M. Note that most elements of M are zero, why is this?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_n6MnwZXZEG"},"outputs":[],"source":["## Task: plot the sparsity pattern of M\n","## note that most elements of M are zero, why is this?\n","\n","\n","\n","##"]},{"cell_type":"markdown","metadata":{"id":"4YcqEx91VjJq"},"source":["### Step 3: Solving the least-squares problem and evaluating performance\n","\n","Finally, we can solve the least-squares system with `jax.numpy.linalg.lstsq`.\n","\n","> Task 6: run the code below to solve the least-squares system"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dahaKzItXZaN"},"outputs":[],"source":["# solve the least-squares problem\n","a = solve_system(M, h, J, K)\n","\n","# run it again for accurate timing (as jit requires a warmup call)\n","start = time.time()\n","a = solve_system(M, h, J, K)\n","a = a.block_until_ready()\n","print(f\"Total training time: {time.time()-start} seconds\")\n","\n","# plot solution\n","i=0\n","u_test_batch = jax.vmap(network.forward, in_axes=(None,None,None,0))(parameters_fixed, tranges, a, t_test_batch)\n","plot_result(i, t_physics_batch, t_boundary, t_test_batch, u_exact_batch, u_test_batch)"]},{"cell_type":"markdown","metadata":{"id":"M4KCGdtpVjMB"},"source":["> Task 7: measure the total training time, and compare this to the JAX FBPINN. Which is faster, and why?\n","\n","> Optional extension tasks. If you have the time, consider these extensions:\n","> 1. Try different hyperparmeters (e.g. number of subdomains, number of basis functions); how sensitive is the solver to them?\n","> 2. Think about how to accelerate the solver even further. One way would be to exploit the sparsity of M; how can this be used to accelerate training further?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsJyDWDyXayE"},"outputs":[],"source":["## Task: measure the total training time, and compare this to the JAX FBPINN. Which is faster, and why?\n","\n","## Task (optional extensions): if you have the time, consider these extensions:\n","    # 1. try different hyperparmeters (e.g. number of subdomains, number of basis functions);\n","    # how sensitive is the solver to them?\n","    # 2. think about how to accelerate the solver even further. One way would be to exploit\n","    # the sparsity of M; how can this be used to accelerate training further?"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}